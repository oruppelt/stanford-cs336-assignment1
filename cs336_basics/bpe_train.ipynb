{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54d2db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned merge: b' ' + b't')\n",
      "Learned merge: b' ' + b'a')\n",
      "Learned merge: b'h' + b'e')\n",
      "Learned merge: b'i' + b'n')\n",
      "Learned merge: b' t' + b'he')\n",
      "Learned merge: b'r' + b'e')\n",
      "Learned merge: b' ' + b'o')\n",
      "Learned merge: b' ' + b',')\n",
      "Learned merge: b'e' + b'r')\n",
      "Learned merge: b' ' + b's')\n",
      "Learned merge: b'a' + b't')\n",
      "Learned merge: b' ' + b'.')\n",
      "Learned merge: b'n' + b'd')\n",
      "Learned merge: b'i' + b's')\n",
      "Learned merge: b'o' + b'r')\n",
      "Learned merge: b' ' + b'w')\n",
      "Learned merge: b' ' + b'c')\n",
      "Learned merge: b'o' + b'n')\n",
      "Learned merge: b' ' + b'b')\n",
      "Learned merge: b' ' + b'f')\n",
      "Learned merge: b'o' + b'u')\n",
      "Learned merge: b'i' + b't')\n",
      "Learned merge: b'e' + b'n')\n",
      "Learned merge: b'e' + b's')\n",
      "Learned merge: b' o' + b'f')\n",
      "Learned merge: b' ' + b'p')\n",
      "Learned merge: b'in' + b'g')\n",
      "Learned merge: b' ' + b'in')\n",
      "Learned merge: b'e' + b'd')\n",
      "Learned merge: b'a' + b'l')\n",
      "Learned merge: b' ' + b'm')\n",
      "Learned merge: b' ' + b'd')\n",
      "Learned merge: b' a' + b'nd')\n",
      "Learned merge: b'a' + b'n')\n",
      "Learned merge: b'a' + b'r')\n",
      "Learned merge: b' t' + b'o')\n",
      "Learned merge: b'o' + b'm')\n",
      "Learned merge: b' t' + b'h')\n",
      "Learned merge: b'i' + b'c')\n",
      "Learned merge: b'i' + b'on')\n",
      "Learned merge: b' ' + b'h')\n",
      "Learned merge: b' ' + b'l')\n",
      "Learned merge: b' ' + b'y')\n",
      "Returned vocab size: 300 | Merges learned: 43\n"
     ]
    }
   ],
   "source": [
    "# bpe_trainer.py\n",
    "# added delta incremental updates for pair frequencies using chatgpt.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import Iterable, List, Tuple, Dict, BinaryIO\n",
    "import heapq\n",
    "\n",
    "import regex as re  # supports \\p{L}, \\p{N}, etc.\n",
    "\n",
    "# ---------- Shared pattern ----------\n",
    "PAT = re.compile(\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")\n",
    "\n",
    "class BPETrainer:\n",
    "    \"\"\"\n",
    "    Owns corpus scanning, pre-tokenization, stats, and merge learning.\n",
    "    Trains byte-level BPE and can export (vocab, merges).\n",
    "    Uses delta updates for pair frequencies (fast).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, byte_level: bool = True, reference_mode: bool = True):\n",
    "        self.PAT = PAT\n",
    "        self.special_tokens: List[str] = [\"<|endoftext|>\"]\n",
    "        self.byte_level = byte_level\n",
    "\n",
    "        # Training state\n",
    "        self.word_counts: Counter[bytes] = Counter()           # token (bytes) -> count\n",
    "        self.segmented_words: Dict[bytes, List[bytes]] = {}    # token -> list of byte symbols\n",
    "        self.vocab_index: Dict[bytes, int] = {}                # symbol -> stable id (non-special)\n",
    "        self.merges: List[Tuple[bytes, bytes]] = []\n",
    "\n",
    "        # Pair-tracking (delta update machinery)\n",
    "        self.pair_freq: Counter[Tuple[bytes, bytes]] = Counter()\n",
    "        self.pair_occ: Dict[Tuple[bytes, bytes], set[bytes]] = defaultdict(set)\n",
    "        self.pair_heap: List[Tuple[int, Tuple[bytes, bytes]]] = []  # (-freq, pair)\n",
    "\n",
    "        # Dirty flag (only for initial build)\n",
    "        self._pairs_dirty = True\n",
    "\n",
    "        # Reference mode (for testing)\n",
    "        # at no point I tested correcness of my implementation against the original BPETrainer\n",
    "        # this is for debugging\n",
    "        self.reference_mode = reference_mode\n",
    "\n",
    "    # ---------- Public pipeline (serial pretokenization) ----------\n",
    "\n",
    "    def pretokenize(self, corpus: Iterable[str]) -> \"BPETrainer\":\n",
    "        \"\"\"\n",
    "        Serial pretokenization: build word_counts (bytes -> freq) skipping special tokens.\n",
    "        \"\"\"\n",
    "        wc: Counter[bytes] = Counter()\n",
    "        specials_s = set(self.special_tokens)\n",
    "\n",
    "        for line in corpus:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            # remove *all* occurrences of each special, not just full-line matches\n",
    "            for s in specials_s:\n",
    "                line = line.replace(s, \" \")\n",
    "\n",
    "            for m in self.PAT.finditer(line):\n",
    "                tok = m.group(0)\n",
    "                if not tok:\n",
    "                    continue\n",
    "                wc[tok.encode(\"utf-8\")] += 1\n",
    "\n",
    "        self._init_state_from_counts(wc)\n",
    "        return self\n",
    "\n",
    "    def compute_pair_stats(self) -> \"BPETrainer\":\n",
    "        \"\"\"\n",
    "        One-time build of pair frequencies, occurrences, and heap from current segmentations.\n",
    "        \"\"\"\n",
    "        import heapq\n",
    "\n",
    "        self.pair_freq.clear()\n",
    "        self.pair_occ.clear()\n",
    "        self.pair_heap.clear()\n",
    "\n",
    "        for w, freq in self.word_counts.items():\n",
    "            seq = self.segmented_words[w]\n",
    "            for a, b in zip(seq, seq[1:]):\n",
    "                p = (a, b)\n",
    "                self.pair_freq[p] += freq\n",
    "                self.pair_occ[p].add(w)\n",
    "\n",
    "        for p, f in self.pair_freq.items():\n",
    "            if f > 0:\n",
    "                heapq.heappush(self.pair_heap, (-f, p))\n",
    "\n",
    "        self._pairs_dirty = False\n",
    "        return self\n",
    "\n",
    "    def fit_to_vocab_size(\n",
    "        self, vocab_size: int, special_tokens: List[str], progress: bool = True\n",
    "    ) -> \"BPETrainer\":\n",
    "        \"\"\"\n",
    "        Greedy BPE loop using delta updates; stops when (256 base + merges) + specials = vocab_size.\n",
    "        \"\"\"\n",
    "\n",
    "        target_non_special = max(0, vocab_size - len(special_tokens))\n",
    "\n",
    "        if self._pairs_dirty or not self.pair_heap:\n",
    "            self.compute_pair_stats()\n",
    "\n",
    "        while len(self.vocab_index) < target_non_special and self.pair_heap:\n",
    "            best_pair = None\n",
    "            # Lazy-pop until top of heap matches current freq (skip stale)\n",
    "            while self.pair_heap:\n",
    "                negf, p = heapq.heappop(self.pair_heap)\n",
    "                f = -negf\n",
    "                if self.pair_freq.get(p, 0) == f and f > 0:\n",
    "                    best_pair = p\n",
    "                    break\n",
    "            if best_pair is None:\n",
    "                break\n",
    "\n",
    "            self._apply_merge_delta(best_pair)\n",
    "            self.merges.append(best_pair)\n",
    "            print(f\"Learned merge: {best_pair[0]} + {best_pair[1]})\")\n",
    "\n",
    "            if progress and (len(self.merges) % 1000 == 0):\n",
    "                pass  # add logging if desired\n",
    "\n",
    "        return self\n",
    "\n",
    "    def export_vocab_and_merges(\n",
    "        self, special_tokens: List[str], vocab_size: int\n",
    "    ) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "        \"\"\"\n",
    "        Build final vocab (IDs -> bytes) with special tokens first, then symbols by stable index.\n",
    "        Caps size at vocab_size.\n",
    "        \"\"\"\n",
    "        vocab: Dict[int, bytes] = {i: s.encode(\"utf-8\") for i, s in enumerate(special_tokens)}\n",
    "        offset = len(special_tokens)\n",
    "\n",
    "        for sym, idx in sorted(self.vocab_index.items(), key=lambda kv: kv[1]):\n",
    "            tid = offset + idx\n",
    "            if tid >= vocab_size:\n",
    "                break\n",
    "            vocab[tid] = sym\n",
    "\n",
    "        return vocab, list(self.merges)\n",
    "\n",
    "    # ---------- Private helpers ----------\n",
    "\n",
    "    def _init_state_from_counts(self, wc: Counter[bytes]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize trainer state from precomputed word_counts (bytes -> freq).\n",
    "        \"\"\"\n",
    "        self.word_counts = wc\n",
    "        # Token -> sequence of single-byte symbols\n",
    "        self.segmented_words = {w: [bytes([b]) for b in w] for w in wc}\n",
    "        # Base 256-byte vocab\n",
    "        base_symbols = [bytes([b]) for b in range(256)]\n",
    "        self.vocab_index = {sym: i for i, sym in enumerate(base_symbols)}\n",
    "        self._pairs_dirty = True\n",
    "\n",
    "    def _apply_merge_delta(self, pair: Tuple[bytes, bytes]) -> None:\n",
    "        \"\"\"\n",
    "        Merge (a,b)->c with incremental updates to pair_freq, pair_occ, and heap.\n",
    "        Only words containing the pair are touched.\n",
    "        \"\"\"\n",
    "\n",
    "        a, b = pair\n",
    "        merged = a + b\n",
    "        if merged not in self.vocab_index:\n",
    "            self.vocab_index[merged] = len(self.vocab_index)\n",
    "\n",
    "        affected = list(self.pair_occ.get(pair, set()))\n",
    "        self.pair_occ[pair].clear()\n",
    "\n",
    "        for w in affected:\n",
    "            seq = self.segmented_words[w]\n",
    "            if not seq:\n",
    "                continue\n",
    "\n",
    "            # Old pairs for this word\n",
    "            old_pairs = list(zip(seq, seq[1:]))\n",
    "\n",
    "            # Apply merge locally\n",
    "            i = 0\n",
    "            new_seq: List[bytes] = []\n",
    "            changed = False\n",
    "            while i < len(seq):\n",
    "                if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                    new_seq.append(merged)\n",
    "                    i += 2\n",
    "                    changed = True\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "\n",
    "            if not changed:\n",
    "                continue  # stale occurrence\n",
    "\n",
    "            # New pairs\n",
    "            new_pairs = list(zip(new_seq, new_seq[1:]))\n",
    "            freq_w = self.word_counts[w]\n",
    "\n",
    "            # Decrement old contributions\n",
    "            for p in old_pairs:\n",
    "                self.pair_freq[p] -= freq_w\n",
    "                if self.pair_freq[p] <= 0:\n",
    "                    self.pair_freq[p] = 0\n",
    "                    # occ cleanup is lazy\n",
    "\n",
    "            # Increment new contributions, update occ and heap lazily\n",
    "            for p in new_pairs:\n",
    "                self.pair_freq[p] += freq_w\n",
    "                if self.pair_freq[p] > 0:\n",
    "                    self.pair_occ[p].add(w)\n",
    "                    heapq.heappush(self.pair_heap, (-self.pair_freq[p], p))\n",
    "\n",
    "            # Save updated segmentation\n",
    "            self.segmented_words[w] = new_seq\n",
    "\n",
    "\n",
    "\n",
    "# -------- Parallel pretokenization helpers (optional) --------\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, desired_num_chunks: int, split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts ending at a delimiter; may return fewer chunks.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes)\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = max(1, file_size // desired_num_chunks)\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 1 << 20  # 1MB probe for fewer syscalls\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        pos = chunk_boundaries[bi]\n",
    "        file.seek(pos)\n",
    "        while True:\n",
    "            buf = file.read(mini_chunk_size)\n",
    "            if buf == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "            j = buf.find(split_special_token)\n",
    "            if j != -1:\n",
    "                chunk_boundaries[bi] = pos + j\n",
    "                break\n",
    "            pos += mini_chunk_size\n",
    "\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "\n",
    "def _count_slice_from_file(path: str, start: int, end: int, specials_s: set[str]) -> Counter[str]:\n",
    "    \"\"\"\n",
    "    Worker: count tokens (string keys) in one slice. Strings are cheaper; convert once later.\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    for s in specials_s:\n",
    "        chunk = chunk.replace(s, \" \")\n",
    "\n",
    "    c = Counter()\n",
    "    for m in PAT.finditer(chunk):\n",
    "        tok = m.group(0)\n",
    "        if tok:\n",
    "            c[tok] += 1\n",
    "    return c\n",
    "\n",
    "\n",
    "def parallel_counts_from_boundaries(\n",
    "    input_path: str, boundaries: list[int], special_tokens: List[str], max_workers: int\n",
    ") -> Counter[bytes]:\n",
    "    \"\"\"\n",
    "    Map→Reduce: produce global word_counts (bytes -> freq) from chunk boundaries.\n",
    "    \"\"\"\n",
    "    totals_str = Counter()\n",
    "    specials_s = set(special_tokens)\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [\n",
    "            ex.submit(_count_slice_from_file, input_path, s, e, specials_s)\n",
    "            for s, e in zip(boundaries[:-1], boundaries[1:])\n",
    "        ]\n",
    "        for fu in futs:\n",
    "            totals_str.update(fu.result())\n",
    "\n",
    "    # Convert keys to bytes once (cheaper than encoding per token)\n",
    "    return Counter({k.encode(\"utf-8\"): v for k, v in totals_str.items()})\n",
    "\n",
    "\n",
    "# -------- Public training function (deliverable) --------\n",
    "\n",
    "def train_bpe_tokenizer(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    "    *,\n",
    "    num_processes: int | None = None,\n",
    "    delimiter: bytes = b\"<|endoftext|>\",\n",
    "    parallel: bool = False,\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    Trains a byte-level BPE tokenizer and returns (vocab_id_to_bytes, merges).\n",
    "\n",
    "    - Ensures base 256-byte alphabet + specials are included within vocab_size.\n",
    "    - If parallel=True, uses chunked pretokenization; otherwise serial.\n",
    "    \"\"\"\n",
    "    required_min = len(special_tokens) + 256\n",
    "    if vocab_size < required_min:\n",
    "        raise ValueError(\n",
    "            f\"vocab_size must be at least {required_min} \"\n",
    "            f\"(= {len(special_tokens)} specials + 256 base bytes)\"\n",
    "        )\n",
    "\n",
    "    trainer = BPETrainer(byte_level=True)\n",
    "    trainer.special_tokens = list(special_tokens)\n",
    "\n",
    "    if parallel:\n",
    "        if num_processes is None:\n",
    "            num_processes = os.cpu_count() or 4\n",
    "        with open(input_path, \"rb\") as f:\n",
    "            boundaries = find_chunk_boundaries(f, num_processes, delimiter)\n",
    "        wc = parallel_counts_from_boundaries(input_path, boundaries, special_tokens, num_processes)\n",
    "        trainer._init_state_from_counts(wc)\n",
    "    else:\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            trainer.pretokenize(f)\n",
    "\n",
    "    trainer.compute_pair_stats().fit_to_vocab_size(vocab_size, special_tokens)\n",
    "    return trainer.export_vocab_and_merges(special_tokens, vocab_size)\n",
    "\n",
    "\n",
    "# ---- Example main (optional) ----\n",
    "if __name__ == \"__main__\":\n",
    "    vocab, merges = train_bpe_tokenizer(\n",
    "        input_path=\"../data/corpus.en\",\n",
    "        vocab_size=300,\n",
    "        special_tokens=[\"<|endoftext|>\"],\n",
    "        parallel=False,\n",
    "        num_processes=4,\n",
    "    )\n",
    "    print(f\"Returned vocab size: {len(vocab)} | Merges learned: {len(merges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c259e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned vocab size: 1000 | Merges learned: 743\n"
     ]
    }
   ],
   "source": [
    "# bpe_trainer.py\n",
    "# added delta incremental updates for pair frequencies using chatgpt.\n",
    "\n",
    "# bpe_trainer.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import Iterable, List, Tuple, Dict, BinaryIO\n",
    "from collections import defaultdict\n",
    "\n",
    "import regex as re  # supports \\p{L}, \\p{N}, etc.\n",
    "\n",
    "# ---------- Shared pattern ----------\n",
    "PAT = re.compile(\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class BPETrainer:\n",
    "    \"\"\"\n",
    "    Owns corpus scanning, pre-tokenization, stats, and merge learning.\n",
    "    Trains byte-level BPE and can export (vocab, merges).\n",
    "    Reference implementation: full recount each step with deterministic tie-break.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, byte_level: bool = True):\n",
    "        self.PAT = PAT\n",
    "        self.special_tokens: List[str] = [\"<|endoftext|>\"]\n",
    "        self.byte_level = byte_level\n",
    "\n",
    "        # Training state\n",
    "        self.word_counts: Counter[bytes] = Counter()           # token (bytes) -> count\n",
    "        self.segmented_words: Dict[bytes, List[bytes]] = {}    # token -> list of byte symbols\n",
    "        self.vocab_index: Dict[bytes, int] = {}                # symbol -> stable id (non-special)\n",
    "        self.merges: List[Tuple[bytes, bytes]] = []\n",
    "\n",
    "        # Pair counts (rebuilt every iteration)\n",
    "        self.pair_counts: Counter[Tuple[bytes, bytes]] = Counter()\n",
    "\n",
    "        self.pair_freq: Counter[tuple[bytes, bytes]] = Counter()      # live pair frequencies\n",
    "        self.pair_occ: dict[tuple[bytes, bytes], set[bytes]] = defaultdict(set)  # which words contain a pair\n",
    "\n",
    "    # ---------- Public pipeline (serial pretokenization) ----------\n",
    "\n",
    "    def pretokenize(self, corpus: Iterable[str]) -> \"BPETrainer\":\n",
    "        \"\"\"\n",
    "        Serial pretokenization: build word_counts (bytes -> freq).\n",
    "        Special-token strings are removed from text so they don't affect merges.\n",
    "        \"\"\"\n",
    "        wc: Counter[bytes] = Counter()\n",
    "        specials_s = set(self.special_tokens)\n",
    "\n",
    "        for line in corpus:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            # Remove any occurrences of special-token strings\n",
    "            for s in specials_s:\n",
    "                line = line.replace(s, \" \")\n",
    "\n",
    "            for m in self.PAT.finditer(line):\n",
    "                tok = m.group(0)\n",
    "                if not tok:\n",
    "                    continue\n",
    "                wc[tok.encode(\"utf-8\")] += 1\n",
    "\n",
    "        self._init_state_from_counts(wc)\n",
    "        return self\n",
    "\n",
    "    def compute_pair_stats(self) -> \"BPETrainer\":\n",
    "        \"\"\"One-time build of pair frequencies and occurrences.\"\"\"\n",
    "        self.pair_freq.clear()\n",
    "        self.pair_occ.clear()\n",
    "        for w, freq in self.word_counts.items():\n",
    "            seq = self.segmented_words[w]\n",
    "            for a, b in zip(seq, seq[1:]):\n",
    "                p = (a, b)\n",
    "                self.pair_freq[p] += freq\n",
    "                self.pair_occ[p].add(w)\n",
    "        return self\n",
    "\n",
    "    def fit_to_vocab_size(self, vocab_size: int, special_tokens: list[str], progress: bool = True) -> \"BPETrainer\":\n",
    "        target_non_special = max(0, vocab_size - len(special_tokens))\n",
    "\n",
    "        # build initial pair stats (once)\n",
    "        if not self.pair_freq:\n",
    "            self.compute_pair_stats()\n",
    "\n",
    "        while len(self.vocab_index) < target_non_special and self.pair_freq:\n",
    "            # choose best by frequency, tie-break by lexicographically GREATER pair\n",
    "            maxf = max(self.pair_freq.values())\n",
    "            # NOTE: max(...) over tuples implements lexicographically greater\n",
    "            best_pair = max((p for p, f in self.pair_freq.items() if f == maxf))\n",
    "\n",
    "            # apply merge with local delta updates\n",
    "            self._apply_merge_delta_simple(best_pair)\n",
    "            self.merges.append(best_pair)\n",
    "\n",
    "            if progress and (len(self.merges) % 1000 == 0):\n",
    "                pass  # add logging if you want\n",
    "\n",
    "        return self\n",
    "\n",
    "    def export_vocab_and_merges(\n",
    "        self, special_tokens: List[str], vocab_size: int\n",
    "    ) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "        \"\"\"\n",
    "        Build final vocab (IDs -> bytes) with special tokens first, then symbols by stable index.\n",
    "        Caps size at vocab_size.\n",
    "        \"\"\"\n",
    "        vocab: Dict[int, bytes] = {i: s.encode(\"utf-8\") for i, s in enumerate(special_tokens)}\n",
    "        offset = len(special_tokens)\n",
    "\n",
    "        for sym, idx in sorted(self.vocab_index.items(), key=lambda kv: kv[1]):\n",
    "            tid = offset + idx\n",
    "            if tid >= vocab_size:\n",
    "                break\n",
    "            vocab[tid] = sym\n",
    "\n",
    "        return vocab, list(self.merges)\n",
    "\n",
    "    # ---------- Private helpers ----------\n",
    "\n",
    "    def _init_state_from_counts(self, wc: Counter[bytes]) -> None:\n",
    "        \"\"\"Initialize trainer state from precomputed word_counts (bytes -> freq).\"\"\"\n",
    "        self.word_counts = wc\n",
    "        # Token -> sequence of single-byte symbols\n",
    "        self.segmented_words = {w: [bytes([b]) for b in w] for w in wc}\n",
    "        # Base 256-byte vocab (ids 0..255 by byte value)\n",
    "        self.vocab_index = {bytes([b]): b for b in range(256)}\n",
    "\n",
    "    def _apply_merge_full(self, pair: Tuple[bytes, bytes]) -> None:\n",
    "        \"\"\"Apply merge (a,b)->a+b to all words (no delta structures).\"\"\"\n",
    "        a, b = pair\n",
    "        merged = a + b\n",
    "        if merged not in self.vocab_index:\n",
    "            self.vocab_index[merged] = len(self.vocab_index)\n",
    "\n",
    "        for w, seq in self.segmented_words.items():\n",
    "            i = 0\n",
    "            out: List[bytes] = []\n",
    "            while i < len(seq):\n",
    "                if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                    out.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    out.append(seq[i])\n",
    "                    i += 1\n",
    "            self.segmented_words[w] = out\n",
    "\n",
    "    def _apply_merge_delta_simple(self, pair: tuple[bytes, bytes]) -> None:\n",
    "        \"\"\"\n",
    "        Merge (a,b)->a+b by updating only words that contain the pair.\n",
    "        Recompute pairs for each affected word and update pair_freq/pair_occ.\n",
    "        \"\"\"\n",
    "        a, b = pair\n",
    "        merged = a + b\n",
    "        if merged not in self.vocab_index:\n",
    "            self.vocab_index[merged] = len(self.vocab_index)\n",
    "\n",
    "        affected_words = list(self.pair_occ.get(pair, set()))\n",
    "        # after we process them, this pair’s occurrences disappear\n",
    "        self.pair_occ[pair].clear()\n",
    "        self.pair_freq[pair] = 0  # its count goes to zero once all replacements are done\n",
    "\n",
    "        for w in affected_words:\n",
    "            seq = self.segmented_words[w]\n",
    "            if not seq:\n",
    "                continue\n",
    "\n",
    "            # ----- old pairs for this word (before) -----\n",
    "            old_pairs = list(zip(seq, seq[1:]))\n",
    "\n",
    "            # ----- apply merge locally -----\n",
    "            i = 0\n",
    "            new_seq: list[bytes] = []\n",
    "            changed = False\n",
    "            while i < len(seq):\n",
    "                if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                    new_seq.append(merged)\n",
    "                    i += 2\n",
    "                    changed = True\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "\n",
    "            if not changed:\n",
    "                # stale membership; skip\n",
    "                continue\n",
    "\n",
    "            # ----- new pairs for this word (after) -----\n",
    "            new_pairs = list(zip(new_seq, new_seq[1:]))\n",
    "            freq_w = self.word_counts[w]\n",
    "\n",
    "            # ----- delta update: remove old contributions -----\n",
    "            for p in old_pairs:\n",
    "                self.pair_freq[p] -= freq_w\n",
    "                if self.pair_freq[p] <= 0:\n",
    "                    # fully remove to keep dict small; also clear occ set\n",
    "                    self.pair_freq.pop(p, None)\n",
    "                    s = self.pair_occ.get(p)\n",
    "                    if s is not None:\n",
    "                        s.discard(w)\n",
    "                        if not s:\n",
    "                            self.pair_occ.pop(p, None)\n",
    "                else:\n",
    "                    # still present globally; ensure w no longer listed for p\n",
    "                    s = self.pair_occ.get(p)\n",
    "                    if s is not None:\n",
    "                        s.discard(w)\n",
    "                        if not s:\n",
    "                            self.pair_occ.pop(p, None)\n",
    "\n",
    "            # ----- delta update: add new contributions -----\n",
    "            for p in new_pairs:\n",
    "                self.pair_freq[p] += freq_w\n",
    "                self.pair_occ.setdefault(p, set()).add(w)\n",
    "\n",
    "            # save updated segmentation\n",
    "            self.segmented_words[w] = new_seq\n",
    "    \n",
    "# -------- Parallel pretokenization helpers (optional) --------\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, desired_num_chunks: int, split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts ending at a delimiter; may return fewer chunks.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes)\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = max(1, file_size // desired_num_chunks)\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 1 << 20  # 1MB probe for fewer syscalls\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        pos = chunk_boundaries[bi]\n",
    "        file.seek(pos)\n",
    "        while True:\n",
    "            buf = file.read(mini_chunk_size)\n",
    "            if buf == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "            j = buf.find(split_special_token)\n",
    "            if j != -1:\n",
    "                chunk_boundaries[bi] = pos + j\n",
    "                break\n",
    "            pos += mini_chunk_size\n",
    "\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "\n",
    "def _count_slice_from_file(path: str, start: int, end: int, specials_s: set[str]) -> Counter[str]:\n",
    "    \"\"\"\n",
    "    Worker: count tokens (string keys) in one slice. Strings are cheaper; convert once later.\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # strip specials so their fragments never appear\n",
    "    for s in specials_s:\n",
    "        chunk = chunk.replace(s, \" \")\n",
    "\n",
    "    c = Counter()\n",
    "    for m in PAT.finditer(chunk):\n",
    "        tok = m.group(0)\n",
    "        if tok:\n",
    "            c[tok] += 1\n",
    "    return c\n",
    "\n",
    "\n",
    "def parallel_counts_from_boundaries(\n",
    "    input_path: str, boundaries: list[int], special_tokens: List[str], max_workers: int\n",
    ") -> Counter[bytes]:\n",
    "    \"\"\"\n",
    "    Map→Reduce: produce global word_counts (bytes -> freq) from chunk boundaries.\n",
    "    \"\"\"\n",
    "    totals_str = Counter()\n",
    "    specials_s = set(special_tokens)\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [\n",
    "            ex.submit(_count_slice_from_file, input_path, s, e, specials_s)\n",
    "            for s, e in zip(boundaries[:-1], boundaries[1:])\n",
    "        ]\n",
    "        for fu in futs:\n",
    "            totals_str.update(fu.result())\n",
    "\n",
    "    # Convert keys to bytes once (cheaper than encoding per token)\n",
    "    return Counter({k.encode(\"utf-8\"): v for k, v in totals_str.items()})\n",
    "\n",
    "\n",
    "# -------- Public training function (deliverable) --------\n",
    "\n",
    "def train_bpe_tokenizer(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    "    *,\n",
    "    num_processes: int | None = None,\n",
    "    delimiter: bytes = b\"<|endoftext|>\",\n",
    "    parallel: bool = False,\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    Trains a byte-level BPE tokenizer and returns (vocab_id_to_bytes, merges).\n",
    "\n",
    "    - Ensures base 256-byte alphabet + specials are included within vocab_size.\n",
    "    - If parallel=True, uses chunked pretokenization; otherwise serial.\n",
    "    \"\"\"\n",
    "    required_min = len(special_tokens) + 256\n",
    "    if vocab_size < required_min:\n",
    "        raise ValueError(\n",
    "            f\"vocab_size must be at least {required_min} \"\n",
    "            f\"(= {len(special_tokens)} specials + 256 base bytes)\"\n",
    "        )\n",
    "\n",
    "    trainer = BPETrainer(byte_level=True)\n",
    "    trainer.special_tokens = list(special_tokens)\n",
    "\n",
    "    if parallel:\n",
    "        if num_processes is None:\n",
    "            num_processes = os.cpu_count() or 4\n",
    "        with open(input_path, \"rb\") as f:\n",
    "            boundaries = find_chunk_boundaries(f, num_processes, delimiter)\n",
    "        wc = parallel_counts_from_boundaries(input_path, boundaries, special_tokens, num_processes)\n",
    "        trainer._init_state_from_counts(wc)\n",
    "    else:\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            trainer.pretokenize(f)\n",
    "\n",
    "    trainer.fit_to_vocab_size(vocab_size, special_tokens)\n",
    "    return trainer.export_vocab_and_merges(special_tokens, vocab_size)\n",
    "\n",
    "\n",
    "# ---- Example main (optional) ----\n",
    "if __name__ == \"__main__\":\n",
    "    vocab, merges = train_bpe_tokenizer(\n",
    "        input_path=\"../data/tinystories_sample_5M.txt\",\n",
    "        vocab_size=1000,\n",
    "        special_tokens=[\"<|endoftext|>\"],\n",
    "        parallel=False,\n",
    "        num_processes=4,\n",
    "    )\n",
    "    print(f\"Returned vocab size: {len(vocab)} | Merges learned: {len(merges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51c6b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_without_specials = [word for word in vocab.values() if word != b\"<|endoftext|>\"]\n",
    "for word_bytes in vocabs_without_specials:\n",
    "    if b\"<|\" in word_bytes:\n",
    "        print(f\"Warning: Found special token in word: {word_bytes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71f924db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'oon'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b165199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'\\x00',\n",
       " b'\\x01',\n",
       " b'\\x02',\n",
       " b'\\x03',\n",
       " b'\\x04',\n",
       " b'\\x05',\n",
       " b'\\x06',\n",
       " b'\\x07',\n",
       " b'\\x08',\n",
       " b'\\t',\n",
       " b'\\n',\n",
       " b'\\x0b',\n",
       " b'\\x0c',\n",
       " b'\\r',\n",
       " b'\\x0e',\n",
       " b'\\x0f',\n",
       " b'\\x10',\n",
       " b'\\x11',\n",
       " b'\\x12',\n",
       " b'\\x13',\n",
       " b'\\x14',\n",
       " b'\\x15',\n",
       " b'\\x16',\n",
       " b'\\x17',\n",
       " b'\\x18',\n",
       " b'\\x19',\n",
       " b'\\x1a',\n",
       " b'\\x1b',\n",
       " b'\\x1c',\n",
       " b'\\x1d',\n",
       " b'\\x1e',\n",
       " b'\\x1f',\n",
       " b' ',\n",
       " b'!',\n",
       " b'\"',\n",
       " b'#',\n",
       " b'$',\n",
       " b'%',\n",
       " b'&',\n",
       " b\"'\",\n",
       " b'(',\n",
       " b')',\n",
       " b'*',\n",
       " b'+',\n",
       " b',',\n",
       " b'-',\n",
       " b'.',\n",
       " b'/',\n",
       " b'0',\n",
       " b'1',\n",
       " b'2',\n",
       " b'3',\n",
       " b'4',\n",
       " b'5',\n",
       " b'6',\n",
       " b'7',\n",
       " b'8',\n",
       " b'9',\n",
       " b':',\n",
       " b';',\n",
       " b'<',\n",
       " b'=',\n",
       " b'>',\n",
       " b'?',\n",
       " b'@',\n",
       " b'A',\n",
       " b'B',\n",
       " b'C',\n",
       " b'D',\n",
       " b'E',\n",
       " b'F',\n",
       " b'G',\n",
       " b'H',\n",
       " b'I',\n",
       " b'J',\n",
       " b'K',\n",
       " b'L',\n",
       " b'M',\n",
       " b'N',\n",
       " b'O',\n",
       " b'P',\n",
       " b'Q',\n",
       " b'R',\n",
       " b'S',\n",
       " b'T',\n",
       " b'U',\n",
       " b'V',\n",
       " b'W',\n",
       " b'X',\n",
       " b'Y',\n",
       " b'Z',\n",
       " b'[',\n",
       " b'\\\\',\n",
       " b']',\n",
       " b'^',\n",
       " b'_',\n",
       " b'`',\n",
       " b'a',\n",
       " b'b',\n",
       " b'c',\n",
       " b'd',\n",
       " b'e',\n",
       " b'f',\n",
       " b'g',\n",
       " b'h',\n",
       " b'i',\n",
       " b'j',\n",
       " b'k',\n",
       " b'l',\n",
       " b'm',\n",
       " b'n',\n",
       " b'o',\n",
       " b'p',\n",
       " b'q',\n",
       " b'r',\n",
       " b's',\n",
       " b't',\n",
       " b'u',\n",
       " b'v',\n",
       " b'w',\n",
       " b'x',\n",
       " b'y',\n",
       " b'z',\n",
       " b'{',\n",
       " b'|',\n",
       " b'}',\n",
       " b'~',\n",
       " b'\\x7f',\n",
       " b'\\x80',\n",
       " b'\\x81',\n",
       " b'\\x82',\n",
       " b'\\x83',\n",
       " b'\\x84',\n",
       " b'\\x85',\n",
       " b'\\x86',\n",
       " b'\\x87',\n",
       " b'\\x88',\n",
       " b'\\x89',\n",
       " b'\\x8a',\n",
       " b'\\x8b',\n",
       " b'\\x8c',\n",
       " b'\\x8d',\n",
       " b'\\x8e',\n",
       " b'\\x8f',\n",
       " b'\\x90',\n",
       " b'\\x91',\n",
       " b'\\x92',\n",
       " b'\\x93',\n",
       " b'\\x94',\n",
       " b'\\x95',\n",
       " b'\\x96',\n",
       " b'\\x97',\n",
       " b'\\x98',\n",
       " b'\\x99',\n",
       " b'\\x9a',\n",
       " b'\\x9b',\n",
       " b'\\x9c',\n",
       " b'\\x9d',\n",
       " b'\\x9e',\n",
       " b'\\x9f',\n",
       " b'\\xa0',\n",
       " b'\\xa1',\n",
       " b'\\xa2',\n",
       " b'\\xa3',\n",
       " b'\\xa4',\n",
       " b'\\xa5',\n",
       " b'\\xa6',\n",
       " b'\\xa7',\n",
       " b'\\xa8',\n",
       " b'\\xa9',\n",
       " b'\\xaa',\n",
       " b'\\xab',\n",
       " b'\\xac',\n",
       " b'\\xad',\n",
       " b'\\xae',\n",
       " b'\\xaf',\n",
       " b'\\xb0',\n",
       " b'\\xb1',\n",
       " b'\\xb2',\n",
       " b'\\xb3',\n",
       " b'\\xb4',\n",
       " b'\\xb5',\n",
       " b'\\xb6',\n",
       " b'\\xb7',\n",
       " b'\\xb8',\n",
       " b'\\xb9',\n",
       " b'\\xba',\n",
       " b'\\xbb',\n",
       " b'\\xbc',\n",
       " b'\\xbd',\n",
       " b'\\xbe',\n",
       " b'\\xbf',\n",
       " b'\\xc0',\n",
       " b'\\xc1',\n",
       " b'\\xc2',\n",
       " b'\\xc3',\n",
       " b'\\xc4',\n",
       " b'\\xc5',\n",
       " b'\\xc6',\n",
       " b'\\xc7',\n",
       " b'\\xc8',\n",
       " b'\\xc9',\n",
       " b'\\xca',\n",
       " b'\\xcb',\n",
       " b'\\xcc',\n",
       " b'\\xcd',\n",
       " b'\\xce',\n",
       " b'\\xcf',\n",
       " b'\\xd0',\n",
       " b'\\xd1',\n",
       " b'\\xd2',\n",
       " b'\\xd3',\n",
       " b'\\xd4',\n",
       " b'\\xd5',\n",
       " b'\\xd6',\n",
       " b'\\xd7',\n",
       " b'\\xd8',\n",
       " b'\\xd9',\n",
       " b'\\xda',\n",
       " b'\\xdb',\n",
       " b'\\xdc',\n",
       " b'\\xdd',\n",
       " b'\\xde',\n",
       " b'\\xdf',\n",
       " b'\\xe0',\n",
       " b'\\xe1',\n",
       " b'\\xe2',\n",
       " b'\\xe3',\n",
       " b'\\xe4',\n",
       " b'\\xe5',\n",
       " b'\\xe6',\n",
       " b'\\xe7',\n",
       " b'\\xe8',\n",
       " b'\\xe9',\n",
       " b'\\xea',\n",
       " b'\\xeb',\n",
       " b'\\xec',\n",
       " b'\\xed',\n",
       " b'\\xee',\n",
       " b'\\xef',\n",
       " b'\\xf0',\n",
       " b'\\xf1',\n",
       " b'\\xf2',\n",
       " b'\\xf3',\n",
       " b'\\xf4',\n",
       " b'\\xf5',\n",
       " b'\\xf6',\n",
       " b'\\xf7',\n",
       " b'\\xf8',\n",
       " b'\\xf9',\n",
       " b'\\xfa',\n",
       " b'\\xfb',\n",
       " b'\\xfc',\n",
       " b'\\xfd',\n",
       " b'\\xfe',\n",
       " b'\\xff',\n",
       " b'he',\n",
       " b' t',\n",
       " b' a',\n",
       " b' s',\n",
       " b' w',\n",
       " b'nd',\n",
       " b' the',\n",
       " b'ed',\n",
       " b' b',\n",
       " b' to',\n",
       " b' and',\n",
       " b' h',\n",
       " b' f',\n",
       " b'in',\n",
       " b' wa',\n",
       " b' T',\n",
       " b'it',\n",
       " b're',\n",
       " b'ou',\n",
       " b' l',\n",
       " b' d',\n",
       " b' c',\n",
       " b' p',\n",
       " b'ay',\n",
       " b' was',\n",
       " b'er',\n",
       " b' m',\n",
       " b'om',\n",
       " b' he',\n",
       " b' The',\n",
       " b'is',\n",
       " b' n',\n",
       " b'on',\n",
       " b'ar',\n",
       " b'im',\n",
       " b' sa',\n",
       " b'll',\n",
       " b'id',\n",
       " b' ha',\n",
       " b' g',\n",
       " b' S',\n",
       " b'at',\n",
       " b'ing',\n",
       " b'ot',\n",
       " b'en',\n",
       " b'an',\n",
       " b'le',\n",
       " b'or',\n",
       " b'ir',\n",
       " b' H',\n",
       " b'am',\n",
       " b'et',\n",
       " b' it',\n",
       " b' th',\n",
       " b'ig',\n",
       " b' They',\n",
       " b'il',\n",
       " b' in',\n",
       " b' He',\n",
       " b' pl',\n",
       " b' \"',\n",
       " b'ow',\n",
       " b'ver',\n",
       " b'ri',\n",
       " b' u',\n",
       " b'ut',\n",
       " b' day',\n",
       " b' said',\n",
       " b'ith',\n",
       " b'pp',\n",
       " b' play',\n",
       " b' be',\n",
       " b'On',\n",
       " b' with',\n",
       " b' o',\n",
       " b' y',\n",
       " b' her',\n",
       " b' r',\n",
       " b'oo',\n",
       " b'ked',\n",
       " b'ce',\n",
       " b' his',\n",
       " b' She',\n",
       " b' I',\n",
       " b'ld',\n",
       " b' st',\n",
       " b' Tim',\n",
       " b'ke',\n",
       " b' e',\n",
       " b'nt',\n",
       " b'ck',\n",
       " b' big',\n",
       " b'very',\n",
       " b' you',\n",
       " b've',\n",
       " b'st',\n",
       " b' happ',\n",
       " b' on',\n",
       " b'end',\n",
       " b'un',\n",
       " b'all',\n",
       " b'ily',\n",
       " b'riend',\n",
       " b' friend',\n",
       " b' L',\n",
       " b' up',\n",
       " b' they',\n",
       " b' want',\n",
       " b' we',\n",
       " b' had',\n",
       " b' not',\n",
       " b'her',\n",
       " b' li',\n",
       " b'itt',\n",
       " b' do',\n",
       " b' of',\n",
       " b' B',\n",
       " b'ad',\n",
       " b'es',\n",
       " b' happy',\n",
       " b' M',\n",
       " b'ent',\n",
       " b' very',\n",
       " b' that',\n",
       " b'One',\n",
       " b' saw',\n",
       " b' mom',\n",
       " b'ould',\n",
       " b\"'s\",\n",
       " b'se',\n",
       " b' for',\n",
       " b'ittle',\n",
       " b' little',\n",
       " b' so',\n",
       " b' she',\n",
       " b' sh',\n",
       " b'ime',\n",
       " b' k',\n",
       " b' time',\n",
       " b' ne',\n",
       " b' nam',\n",
       " b'ch',\n",
       " b' there',\n",
       " b'.\"',\n",
       " b'ound',\n",
       " b' bo',\n",
       " b' named',\n",
       " b' Lily',\n",
       " b'ird',\n",
       " b' sm',\n",
       " b' wanted',\n",
       " b' bird',\n",
       " b' Tom',\n",
       " b'ved',\n",
       " b' were',\n",
       " b' but',\n",
       " b'The',\n",
       " b' friends',\n",
       " b'!\"',\n",
       " b'out',\n",
       " b'ht',\n",
       " b'el',\n",
       " b'ake',\n",
       " b'al',\n",
       " b' an',\n",
       " b' too',\n",
       " b'ide',\n",
       " b' hel',\n",
       " b'Once',\n",
       " b' wh',\n",
       " b' help',\n",
       " b'ug',\n",
       " b'ome',\n",
       " b' went',\n",
       " b' all',\n",
       " b' It',\n",
       " b' A',\n",
       " b' loo',\n",
       " b' is',\n",
       " b' upon',\n",
       " b' lo',\n",
       " b'ue',\n",
       " b'ter',\n",
       " b'ry',\n",
       " b'ore',\n",
       " b'ind',\n",
       " b'get',\n",
       " b' fun',\n",
       " b' toy',\n",
       " b'ill',\n",
       " b'ack',\n",
       " b'gether',\n",
       " b' at',\n",
       " b' j',\n",
       " b' as',\n",
       " b' did',\n",
       " b'ame',\n",
       " b' re',\n",
       " b' together',\n",
       " b'ur',\n",
       " b' cat',\n",
       " b'ra',\n",
       " b' dog',\n",
       " b' se',\n",
       " b'ly',\n",
       " b' tre',\n",
       " b'ood',\n",
       " b' gir',\n",
       " b' can',\n",
       " b'ic',\n",
       " b' ball',\n",
       " b'my',\n",
       " b' girl',\n",
       " b' could',\n",
       " b'ard',\n",
       " b'ted',\n",
       " b' him',\n",
       " b' their',\n",
       " b'ec',\n",
       " b'ark',\n",
       " b' ro',\n",
       " b'way',\n",
       " b' kn',\n",
       " b' played',\n",
       " b'?\"',\n",
       " b'hed',\n",
       " b' fr',\n",
       " b'ain',\n",
       " b'hen',\n",
       " b' le',\n",
       " b' out',\n",
       " b'um',\n",
       " b\"'t\",\n",
       " b'ax',\n",
       " b' go',\n",
       " b' them',\n",
       " b' are',\n",
       " b' boy',\n",
       " b' sad',\n",
       " b'ul',\n",
       " b'oug',\n",
       " b' have',\n",
       " b' tree',\n",
       " b' cl',\n",
       " b' loved',\n",
       " b'other',\n",
       " b' back',\n",
       " b' looked',\n",
       " b' sp',\n",
       " b' found',\n",
       " b'own',\n",
       " b'one',\n",
       " b' Max',\n",
       " b' me',\n",
       " b' sc',\n",
       " b' man',\n",
       " b'are',\n",
       " b' star',\n",
       " b' Sue',\n",
       " b'hing',\n",
       " b'side',\n",
       " b' park',\n",
       " b'ong',\n",
       " b'op',\n",
       " b'ful',\n",
       " b' bec',\n",
       " b' like',\n",
       " b' would',\n",
       " b' liked',\n",
       " b'elt',\n",
       " b'round',\n",
       " b' One',\n",
       " b' felt',\n",
       " b'ight',\n",
       " b' fa',\n",
       " b' Ben',\n",
       " b' la',\n",
       " b' W',\n",
       " b'ell',\n",
       " b' make',\n",
       " b' But',\n",
       " b'You',\n",
       " b' asked',\n",
       " b' new',\n",
       " b'ice',\n",
       " b'iled',\n",
       " b'ss',\n",
       " b'ared',\n",
       " b'ag',\n",
       " b' Sam',\n",
       " b'ought',\n",
       " b' see',\n",
       " b'omet',\n",
       " b' al',\n",
       " b' started',\n",
       " b' ag',\n",
       " b' came',\n",
       " b' no',\n",
       " b' smiled',\n",
       " b' car',\n",
       " b' other',\n",
       " b'ried',\n",
       " b' somet',\n",
       " b' good',\n",
       " b' say',\n",
       " b' br',\n",
       " b' small',\n",
       " b'ade',\n",
       " b'pot',\n",
       " b'ob',\n",
       " b'ings',\n",
       " b' wor',\n",
       " b' away',\n",
       " b'ab',\n",
       " b'ouse',\n",
       " b' find',\n",
       " b'us',\n",
       " b' ex',\n",
       " b' from',\n",
       " b' put',\n",
       " b'ty',\n",
       " b' thought',\n",
       " b' what',\n",
       " b' made',\n",
       " b'ened',\n",
       " b' playing',\n",
       " b'ie',\n",
       " b' home',\n",
       " b'king',\n",
       " b' wal',\n",
       " b'ach',\n",
       " b' something',\n",
       " b'ia',\n",
       " b'ile',\n",
       " b' every',\n",
       " b' ran',\n",
       " b' Spot',\n",
       " b' again',\n",
       " b' F',\n",
       " b' co',\n",
       " b'ave',\n",
       " b'uc',\n",
       " b'arn',\n",
       " b' box',\n",
       " b' down',\n",
       " b'dd',\n",
       " b' toys',\n",
       " b'ook',\n",
       " b' J',\n",
       " b' pr',\n",
       " b' took',\n",
       " b' sw',\n",
       " b' some',\n",
       " b'if',\n",
       " b' laug',\n",
       " b'oud',\n",
       " b'ust',\n",
       " b'ew',\n",
       " b' sun',\n",
       " b' around',\n",
       " b'ny',\n",
       " b' scared',\n",
       " b' lived',\n",
       " b' learn',\n",
       " b' will',\n",
       " b' my',\n",
       " b' fl',\n",
       " b'ure',\n",
       " b' your',\n",
       " b'ick',\n",
       " b'ret',\n",
       " b'ep',\n",
       " b' house',\n",
       " b' when',\n",
       " b' You',\n",
       " b' things',\n",
       " b' bl',\n",
       " b',\"',\n",
       " b'ank',\n",
       " b'uck',\n",
       " b' then',\n",
       " b'pped',\n",
       " b' Mia',\n",
       " b'ist',\n",
       " b'Th',\n",
       " b' bot',\n",
       " b' ab',\n",
       " b'ish',\n",
       " b' who',\n",
       " b' get',\n",
       " b' know',\n",
       " b'ucy',\n",
       " b' fe',\n",
       " b'as',\n",
       " b' tried',\n",
       " b' got',\n",
       " b' knew',\n",
       " b'ited',\n",
       " b' lot',\n",
       " b'uch',\n",
       " b' says',\n",
       " b'ally',\n",
       " b'ap',\n",
       " b' int',\n",
       " b' ch',\n",
       " b'ive',\n",
       " b' red',\n",
       " b'Tim',\n",
       " b'nder',\n",
       " b'ump',\n",
       " b'Lily',\n",
       " b' pret',\n",
       " b'ous',\n",
       " b' about',\n",
       " b' care',\n",
       " b'fter',\n",
       " b' dec',\n",
       " b' So',\n",
       " b' op',\n",
       " b'fe',\n",
       " b' look',\n",
       " b' D',\n",
       " b' many',\n",
       " b'use',\n",
       " b' un',\n",
       " b' more',\n",
       " b'ise',\n",
       " b'ways',\n",
       " b' E',\n",
       " b'nn',\n",
       " b' exc',\n",
       " b' Mom',\n",
       " b' always',\n",
       " b' pic',\n",
       " b' po',\n",
       " b'\\xe2\\x80',\n",
       " b' best',\n",
       " b'qu',\n",
       " b' any',\n",
       " b' show',\n",
       " b' both',\n",
       " b' Lucy',\n",
       " b'ite',\n",
       " b' room',\n",
       " b' hug',\n",
       " b'ace',\n",
       " b'ers',\n",
       " b'ause',\n",
       " b' into',\n",
       " b' water',\n",
       " b' became',\n",
       " b' outside',\n",
       " b' learned',\n",
       " b' laughed',\n",
       " b'udd',\n",
       " b' dad',\n",
       " b'ided',\n",
       " b'ant',\n",
       " b' because',\n",
       " b' one',\n",
       " b' gre',\n",
       " b' pe',\n",
       " b' decided',\n",
       " b' And',\n",
       " b'pr',\n",
       " b' excited',\n",
       " b' v',\n",
       " b'Yes',\n",
       " b' nice',\n",
       " b' ho',\n",
       " b'nna',\n",
       " b' ke',\n",
       " b' eat',\n",
       " b' sor',\n",
       " b' this',\n",
       " b'ara',\n",
       " b' jump',\n",
       " b' long',\n",
       " b' feel',\n",
       " b'They',\n",
       " b' Bob',\n",
       " b'Tom',\n",
       " b' mo',\n",
       " b' told',\n",
       " b' run',\n",
       " b'ull',\n",
       " b'ink',\n",
       " b' sur',\n",
       " b' rock',\n",
       " b' inside',\n",
       " b' pretty',\n",
       " b' sl',\n",
       " b' sk',\n",
       " b'lew',\n",
       " b'iny',\n",
       " b' take',\n",
       " b' am',\n",
       " b' gave',\n",
       " b' surpr',\n",
       " b' fast',\n",
       " b' each',\n",
       " b' tow',\n",
       " b'our',\n",
       " b'ess',\n",
       " b'ro',\n",
       " b'ven',\n",
       " b'ged',\n",
       " b' sorry',\n",
       " b' need',\n",
       " b' or',\n",
       " b' much',\n",
       " b' than',\n",
       " b'urt',\n",
       " b' how',\n",
       " b'ised',\n",
       " b' str',\n",
       " b'able',\n",
       " b' under',\n",
       " b'Wh',\n",
       " b'og',\n",
       " b'ge',\n",
       " b' kind',\n",
       " b'and',\n",
       " b' gra',\n",
       " b' Amy',\n",
       " b'imal',\n",
       " b'ving',\n",
       " b' animal',\n",
       " b'ft',\n",
       " b' ta',\n",
       " b'But',\n",
       " b' food',\n",
       " b'etter',\n",
       " b'Can',\n",
       " b' walked',\n",
       " b' We',\n",
       " b' tr',\n",
       " b' wat',\n",
       " b' hand',\n",
       " b' old',\n",
       " b' His',\n",
       " b' list',\n",
       " b' hig',\n",
       " b'ff',\n",
       " b' just',\n",
       " b' clo',\n",
       " b' flow',\n",
       " b' en',\n",
       " b'rom',\n",
       " b' cle',\n",
       " b'ched',\n",
       " b' didn',\n",
       " b'urn',\n",
       " b' ide',\n",
       " b'eci',\n",
       " b'Mom',\n",
       " b'ase',\n",
       " b' try',\n",
       " b' idea',\n",
       " b' dan',\n",
       " b'ion',\n",
       " b'here',\n",
       " b' sky',\n",
       " b' its',\n",
       " b'ecial',\n",
       " b' special',\n",
       " b' near',\n",
       " b' flew',\n",
       " b' us',\n",
       " b' Anna',\n",
       " b' tw',\n",
       " b' happened',\n",
       " b'pl',\n",
       " b'gry',\n",
       " b' fo',\n",
       " b'ine',\n",
       " b' fi',\n",
       " b' sn',\n",
       " b' Th',\n",
       " b'Thank',\n",
       " b' if',\n",
       " b' cake',\n",
       " b'ber',\n",
       " b' fish',\n",
       " b' animals',\n",
       " b' stor',\n",
       " b' over',\n",
       " b' bug',\n",
       " b'lf',\n",
       " b' heard',\n",
       " b'ate',\n",
       " b' careful',\n",
       " b'ex',\n",
       " b' com',\n",
       " b'udden',\n",
       " b'rm',\n",
       " b' better',\n",
       " b' lots',\n",
       " b' by',\n",
       " b' let',\n",
       " b' hard',\n",
       " b' wind',\n",
       " b' never',\n",
       " b' hurt',\n",
       " b' bear',\n",
       " b' share',\n",
       " b'ople',\n",
       " b' Every',\n",
       " b' dre',\n",
       " b'uddenly',\n",
       " b'more',\n",
       " b' anymore',\n",
       " b' even',\n",
       " b' te',\n",
       " b'ort',\n",
       " b' tal',\n",
       " b'pec',\n",
       " b' fly',\n",
       " b' Sara',\n",
       " b' safe',\n",
       " b' love',\n",
       " b'ady',\n",
       " b' bu',\n",
       " b' end',\n",
       " b' don',\n",
       " b' K',\n",
       " b' fore',\n",
       " b' give',\n",
       " b' people',\n",
       " b' after',\n",
       " b' clean',\n",
       " b'uff',\n",
       " b' proud',\n",
       " b' opened',\n",
       " b'bed',\n",
       " b\"'m\",\n",
       " b' high',\n",
       " b' gr',\n",
       " b' fam',\n",
       " b' bad',\n",
       " b' door',\n",
       " b' surprised',\n",
       " b' Then',\n",
       " b'itty',\n",
       " b'ak',\n",
       " b' turn',\n",
       " b'As',\n",
       " b' gl',\n",
       " b'age',\n",
       " b' From',\n",
       " b'ummy',\n",
       " b'ough',\n",
       " b' ground',\n",
       " b' clim',\n",
       " b' Her',\n",
       " b' thanked',\n",
       " b' im',\n",
       " b' When',\n",
       " b'iz',\n",
       " b' blue',\n",
       " b' call',\n",
       " b' book',\n",
       " b'kay',\n",
       " b'arden',\n",
       " b' garden',\n",
       " b'ild',\n",
       " b' far',\n",
       " b' ma',\n",
       " b' ever',\n",
       " b' way',\n",
       " b'pected',\n",
       " b' hugged',\n",
       " b' che',\n",
       " b' shiny',\n",
       " b' col',\n",
       " b' picked',\n",
       " b'Let',\n",
       " b' smile',\n",
       " b' mag',\n",
       " b' loud',\n",
       " b' cu',\n",
       " b' off',\n",
       " b' should',\n",
       " b' come',\n",
       " b' In',\n",
       " b'expected',\n",
       " b' unexpected',\n",
       " b'xt',\n",
       " b' next',\n",
       " b' stick',\n",
       " b' place',\n",
       " b' walk',\n",
       " b' N',\n",
       " b'ture',\n",
       " b' family',\n",
       " b'hes',\n",
       " b'em',\n",
       " b' qu',\n",
       " b' color',\n",
       " b'by',\n",
       " b' P',\n",
       " b' still',\n",
       " b'ip',\n",
       " b' unt',\n",
       " b' strong',\n",
       " b' great',\n",
       " b' boat',\n",
       " b' app',\n",
       " b'les',\n",
       " b' hat',\n",
       " b'oy',\n",
       " b' happily',\n",
       " b' bro',\n",
       " b'ool',\n",
       " b' until',\n",
       " b' jumped',\n",
       " b' town',\n",
       " b' ca',\n",
       " b' frog',\n",
       " b'dy',\n",
       " b' kid',\n",
       " b' helped',\n",
       " b' Sally',\n",
       " b' forest',\n",
       " b'ies',\n",
       " b' sto',\n",
       " b'orn',\n",
       " b'ct',\n",
       " b' doll',\n",
       " b'aut',\n",
       " b' squ',\n",
       " b'ary',\n",
       " b' Kitty',\n",
       " b' beaut',\n",
       " b' listen',\n",
       " b' showed',\n",
       " b' met',\n",
       " b' now',\n",
       " b'No',\n",
       " b'uffy',\n",
       " b' par',\n",
       " b' magic',\n",
       " b'oon']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs_without_specials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd655cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = FIXTURES_PATH / \"tinystories_sample_5M.txt\"\n",
    "vocab, merges = train_bpe_tokenizer(\n",
    "    input_path=\"../data/tinystories_sample_5M.txt\",\n",
    "    vocab_size=1000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "# save vocab to disk for debugging\n",
    "# with open(\"train-bpe-special-tokens-vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "# Check that the special token is not in the vocab\n",
    "vocabs_without_specials = [word for word in vocab.values() if word != b\"<|endoftext|>\"]\n",
    "for word_bytes in vocabs_without_specials:\n",
    "    if b\"<|\" in word_bytes:\n",
    "        print(f\"Warning: Found special token in word: {word_bytes}\")\n",
    "    assert b\"<|\" not in word_bytes\n",
    "\n",
    "# snapshot.assert_match(\n",
    "#     {\n",
    "#         \"vocab_keys\": set(vocab.keys()),\n",
    "#         \"vocab_values\": set(vocab.values()),\n",
    "#         \"merges\": merges,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5f587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
