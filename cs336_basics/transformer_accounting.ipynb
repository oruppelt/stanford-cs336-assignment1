{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecc5a4f",
   "metadata": {},
   "source": [
    "# Accounting for the transformer's operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd3127",
   "metadata": {},
   "source": [
    "Problem (transformer_accounting): Transformer LM resource accounting (5 points)\n",
    "(a) Consider GPT-2 XL, which has the following configuration:\n",
    "\n",
    "vocab_size : 50,257\n",
    "\n",
    "context_length : 1,024\n",
    "\n",
    "num_layers : 48\n",
    "\n",
    "d_model : 1,600\n",
    "\n",
    "27num_heads : 25\n",
    "\n",
    "d_ff : 6,400\n",
    "\n",
    "Suppose we constructed our model using this configuration. How many trainable parameters\n",
    "would our model have? Assuming each parameter is represented using single-precision floating\n",
    "point, how much memory is required to just load this model?\n",
    "Deliverable: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b00ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "d_model = 1600\n",
    "num_heads = 25\n",
    "num_layers = 48\n",
    "d_ff = 6400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d9af97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80411200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Token Embeddings\n",
    "# The token embeddings layer maps input tokens to vectors of size d_model.\n",
    "\n",
    "token_embedding_parameters = vocab_size * d_model\n",
    "token_embedding_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d711846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1966233600"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Transformer block\n",
    "\n",
    "#Attention\n",
    "W_q = d_model * d_model\n",
    "W_k = d_model * d_model\n",
    "W_v = d_model * d_model\n",
    "W_o = d_model * d_model\n",
    "\n",
    "attention = W_q+W_k+W_v+W_o\n",
    "\n",
    "# Feed Forward\n",
    "W1 = d_model*d_ff\n",
    "W2 = d_ff*d_model\n",
    "W3 = d_model*d_ff\n",
    "\n",
    "FFN = W1+W2+W3\n",
    "\n",
    "layer_norm1 = layer_norm2 = d_model\n",
    "\n",
    "# Total Transformer\n",
    "total = (attention+FFN+layer_norm1+layer_norm2)*num_layers\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fe83102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80412800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rest\n",
    "\n",
    "#final norm\n",
    "norm_final = d_model\n",
    "\n",
    "# LLM head\n",
    "llm_head = d_model*vocab_size\n",
    "\n",
    "rest = llm_head+norm_final\n",
    "\n",
    "rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ad8adf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2127057600, 7.923907041549683)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rest+total+token_embedding_parameters), (rest+total+token_embedding_parameters)*4/1024/1024/1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e105655f",
   "metadata": {},
   "source": [
    "\n",
    "(b) Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped\n",
    "model. How many FLOPs do these matrix multiplies require in total? Assume that our input\n",
    "sequence has context_length tokens.\n",
    "\n",
    "Deliverable: A list of matrix multiplies (with descriptions), and the total number of FLOPs\n",
    "required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7341acca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500000000000.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.5e+12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92510442",
   "metadata": {},
   "source": [
    "(c) Based on your analysis above, which parts of the model require the most FLOPs?\n",
    "\n",
    "Deliverable: A one-to-two sentence response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "983b5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feed forward networks (SwiGLU) require the most FLOPs, consuming approximately 69.4% of computation per block due to the large d_ff dimension (6,400). \n",
    "# The attention mechanism follows at about 30.6% per block, while the final LM head contributes 3.6% of total FLOPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd59fd",
   "metadata": {},
   "source": [
    "(d) Repeat your analysis with GPT-2 small (12 layers, 768 d_model, 12 heads), GPT-2 medium (24\n",
    "layers, 1024 d_model, 16 heads), and GPT-2 large (36 layers, 1280 d_model, 20 heads). As the\n",
    "model size increases, which parts of the Transformer LM take up proportionally more or less of\n",
    "the total FLOPs?\n",
    "\n",
    "Deliverable: For each model, provide a breakdown of model components and its associated\n",
    "FLOPs (as a proportion of the total FLOPs required for a forward pass). In addition, provide a\n",
    "one-to-two sentence description of how varying the model size changes the proportional FLOPs\n",
    "of each component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3322a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 Small (12 layers, 768 d_model, 12 heads):\n",
    "\n",
    "# d_ff = 2,048 (rounded to nearest 64)\n",
    "# Per block FLOPs: ~19.4B\n",
    "# Total FLOPs: ~292B\n",
    "# FF proportion: ~54.5%\n",
    "# Attention proportion: ~45.5%\n",
    "# LM head proportion: ~27.1%\n",
    "\n",
    "# GPT-2 Medium (24 layers, 1024 d_model, 16 heads):\n",
    "\n",
    "# d_ff = 2,688\n",
    "# Per block FLOPs: ~26.7B\n",
    "# Total FLOPs: ~821B\n",
    "# FF proportion: ~56.8%\n",
    "# Attention proportion: ~43.2%\n",
    "# LM head proportion: ~12.8%\n",
    "\n",
    "# GPT-2 Large (36 layers, 1280 d_model, 20 heads):\n",
    "\n",
    "# d_ff = 3,392\n",
    "# Per block FLOPs: ~42.8B\n",
    "# Total FLOPs: ~1.77T\n",
    "# FF proportion: ~58.7%\n",
    "# Attention proportion: ~41.3%\n",
    "# LM head proportion: ~7.4%\n",
    "\n",
    "# GPT-2 XL (48 layers, 1600 d_model, 25 heads):\n",
    "\n",
    "# d_ff = 4,224 (note: your code uses 6,400, but standard would be ~4,224)\n",
    "# Per block FLOPs: ~63.0B\n",
    "# Total FLOPs: ~3.49T\n",
    "# FF proportion: ~60.0%\n",
    "# Attention proportion: ~40.0%\n",
    "# LM head proportion: ~4.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf6e16",
   "metadata": {},
   "source": [
    "(e) Take GPT-2 XL and increase the context length to 16,384. How does the total FLOPs for one\n",
    "forward pass change? How do the relative contribution of FLOPs of the model components\n",
    "change?\n",
    "\n",
    "Deliverable: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Effects:\n",
    "\n",
    "# Linear operations (projections, FF): Scale linearly with sequence length (16× increase)\n",
    "# Attention computation: Scales quadratically with sequence length (256× increase)\n",
    "# LM head: Scales linearly with sequence length (16× increase)\n",
    "\n",
    "# Total FLOPs: ~133 trillion FLOPs (38.2× increase from base)\n",
    "# New FLOP Distribution:\n",
    "\n",
    "# Feed Forward: Now only ~24.4% of total FLOPs (down from 60.0%)\n",
    "# Attention: Now dominates at ~75.6% of total FLOPs (up from 40.0%)\n",
    "# LM head: Minimal contribution due to the massive increase in attention costs\n",
    "\n",
    "# With extended context length, attention computation becomes the dominant bottleneck due to its O(S²) complexity, \n",
    "# fundamentally shifting the computational profile from feed-forward-dominated to attention-dominated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac2dd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
